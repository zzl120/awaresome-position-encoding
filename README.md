# awaresome-position-encoding
|Paper|URL|Year|
|----|----|----|
|Self-Attention with Relative Position Representations|[URL](https://arxiv.org/abs/1803.02155)|2018|
|Rethinking Positional Encoding in Language Pre-training|[URL](https://arxiv.org/abs/2006.15595)|2020|
|Conditional DETR for Fast Training Convergence|[URL](https://arxiv.org/abs/2108.06152)|2021|
|RoFormer: Enhanced Transformer with Rotary Position Embedding|[URL](https://arxiv.org/abs/2104.09864)|2021|
|Encoding word order in complex embeddings|[URL](https://arxiv.org/abs/1912.12333)|2019|
|Conditional Positional Encodings for Vision Transformers|[URL](https://arxiv.org/abs/2102.10882)|2021|
|Learning to Encode Position for Transformer with Continuous Dynamical Model|[URL](https://arxiv.org/abs/2003.09229)|2020|
|Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context|[URL](https://arxiv.org/abs/1901.02860)|2019|
